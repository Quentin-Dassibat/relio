# Copyright (c) Quentin DASSIBAT <qdassibat@gmail.com>

#Ecole des Mines de Saint-Etienne (EMSE)
#Ecole Nationale des Travaux Publics de l'Etat (ENTPE)
#Ecole Urbaine de Lyon (EUL)

# Source Code License (GPLv3)

#This software and its source code are licensed under the GNU General Public License (GPL), version 3.0 or later. See the LICENSE file for details.

# Output License (CC BY 4.0)

#Any outputs generated by this software, such as data files, images, or other results, are licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0).

#You are free to share, adapt, and use these outputs, provided you give appropriate credit to the original author(s). See the LICENSE file for details.

# For details about each license, please refer to:
#- GNU General Public License (GPL), version 3.0: https://www.gnu.org/licenses/gpl-3.0.html
#- Creative Commons Attribution 4.0 International License (CC BY 4.0): http://creativecommons.org/licenses/by/4.0/ 



################################################### 
# REsampling Large Inputs/Outputs (relio) Library #
###################################################




def reproject(srcFile,dstFile,srcEPSG,dstEPSG):

    """
    srcFile: path/to/source/raster/file.tif [string]
    dstFile: path/to/destination/raster/file.tif [string] (this function assumes srcFile and dstFile have same resolution)
    srcEPSG: EPSG code of source file [int]
    srcEPSG: EPSG code of source file [int]

    output : reprojected raster
    """
    
    import os
    from osgeo import gdal
    
    cmd = f"gdalwarp -overwrite -s_srs EPSG:{str(srcEPSG)} -t_srs EPSG:{str(dstEPSG)} -r near -of GTiff {str(srcFile)} {str(dstFile)}"
    os.system(cmd)

    return

def clip(srcFile,dstFile,EPSG,bbox):

    """
    srcFile: path/to/source/raster/file.tif [string]
    dstFile: path/to/destination/raster/file.tif [string] (dstFile will be given the roslution of srcFile)
    EPSG: single EPSG code for both source and destination files [int]
    bbox: "xmin ymin xmax ymax" [string]

    output : cropped raster 

    warning: gdalwarp produces an offset from srcFile to dstFile (for the tested rasters: by -0.04 km in Y and -3 km in X)
    """

    import os
    from osgeo import gdal
    
    cmd = f"gdalwarp -overwrite -s_srs EPSG:{str(EPSG)} -t_srs EPSG:{str(EPSG)} -te {str(bbox)} -te_srs EPSG:{str(EPSG)} -ot Float32  -r near -of GTiff {srcFile} {dstFile}"
    os.system(cmd)

    return

def clip_to_shapefile(srcFile,dstFile,EPSG,maskFile):

    """
    srcFile: path/to/source/raster/file.tif [string]
    dstFile: path/to/destination/raster/file.tif [string]
    maskFile: path/to/mask/raster/file.shp or any osgeo-compatible vector format [string]
    EPSG: epsg code of both src, mask and dst (must be the same) [int]
    """

    import os
    import numpy as np

    #Extract layerName from maskFile
    a = maskFile
    b = np.char.split(a, sep ='/') 
    c = np.ndarray.tolist(b)
    d = c[-1]
    layerName = d[:-4]

    #Execute gdalwarp
    cmd = f"gdalwarp -overwrite -s_srs EPSG:{str(EPSG)} -t_srs EPSG:{str(EPSG)} -of GTiff -cutline {str(maskFile)} -cl {str(layerName)} -crop_to_cutline {str(srcFile)} {str(dstFile)}"
    os.system(cmd)

    return



def merge_rasters(fileList,dstFile,maskFile):

    """
    fileList: /path/to/text/file.txt containing paths to the raster .tif files (one and only one path for each line in text file) [string]
    dstFile: /path/to/merged/raster.vrt [string]
    maskFle: /path/to/mask/file.tif is the raster file used to retrieve geotransform properties to be applied to dstFile [string]
    epsgCode : EPSG code of both src, mask and dst rasters [int]
    """

    import os
    from osgeo import gdal
    gdal.UseExceptions()
    
    #Read maskfile to get geotransfrom info
    mask = gdal.Open(maskFile)
    upx, xres, xskew, upy, yskew, yres = mask.GetGeoTransform()
    cols = mask.RasterXSize
    rows = mask.RasterYSize
    llx = upx + 0*xres + rows*xskew
    lly = upy + 0*yskew + rows*yres
    urx = upx + cols*xres + 0*xskew
    ury = upy + cols*yskew + 0*yres
    mask = None

    #tmpFile = f"{dstFile[:-4]}_tmp.vrt"
    #vrt = gdal.BuildVRT(dstFile, fileList)
    #vrt = None
    
    cmd = f"gdal_merge.py -ps {xres} {yres} -ul_lr {llx} {ury} {urx} {lly} -ot Float32 -of GTiff -o {dstFile} --optfile {fileList}"
    os.system(cmd)

    return 
    
def merge_SameSizedRasters(rastersList,dstFile,epsgCode):
    
    """
    """

    #Import libraries

    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    gdalDataTypes = {
                      "uint8": 1,
                      "int8": 1,
                      "uint16": 2,
                      "int16": 3,
                      "uint32": 4,
                      "int32": 5,
                      "float32": 6,
                      "float64": 7,
                      "complex64": 10,
                      "complex128": 11
                    } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read first file of rastersList and extract geoT info

    r = gdal.Open(rastersList[0])
    band = r.GetRasterBand(1)
    src = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    
    h,w = src.shape
    
    r = None

    #Create an empty array populated with NaN values to further store results
    
    array = np.empty([h,w])
    array[:] = np.float32(np.nan)
    
    #Range over each raster given in rastersList and retrieve its cell value when it is not NaN
    
    for raster in range(len(rastersList)):
        
        #Open the raster 

        r = gdal.Open(rastersList[raster])
        band = r.GetRasterBand(1)
        src = band.ReadAsArray().astype(float)

        #Range over cells and retrieve cells on condition

        for y in range(h): 

            if y == 0: #yres is negative and y axis goes x-wise
                ymax = uly 
                ymin = uly + yres
            else:
                ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
                ymin = uly + y * yres + yres 

            for x in range(w):

                if x == 0:
                    xmin = ulx 
                    xmax = ulx + xres
                else:
                    xmin = ulx + x * xres 
                    xmax = ulx + x * xres + xres

                if (xmin >= xmax) | (ymin >= ymax):
                    print("Error (x,y)",(x,y))
                else:
                    pass

                #Set float value
                tmp = np.float32(src[y,x])

                #Retieve on condition
                
                if np.isnan(tmp):
                    pass 
                else:
                    array[y,x] = tmp
                
        r = None    

    #Export array as dstFile .tif
    
    gdalType = gdalDataTypes[array.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, gdalType)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(array)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None
    
    return



def force_resolution(srcFile,dstFile,maskFile,epsgCode):

    """
    """

    import os
    from osgeo import gdal
    gdal.UseExceptions()
    
    #Read maskfile to get geotransfrom info
    mask = gdal.Open(maskFile)
    upx, xres, xskew, upy, yskew, yres = mask.GetGeoTransform()
    cols = mask.RasterXSize
    rows = mask.RasterYSize
    llx = upx + 0*xres + rows*xskew
    lly = upy + 0*yskew + rows*yres
    urx = upx + cols*xres + 0*xskew
    ury = upy + cols*yskew + 0*yres
    mask = None

    #Warp file
    tr = f"{str(xres)} {str(yres)}"
    te = f"{str(llx)} {str(lly)} {str(urx)} {str(ury)}"
    ts = f"{str(cols)} {str(rows)}"
    cmd = f"gdalwarp -overwrite -s_srs EPSG:{str(epsgCode)} -t_srs EPSG:{str(epsgCode)} -ts {str(ts)} -tr {str(tr)} -te {str(te)} -r near -of VRT {str(srcFile)} {str(dstFile)}"
    os.system(cmd)

    return


def extract_Rasterbbox(srcFile):

    """
    srcFile: path/to/source/raster/file.tif [string]

    output : bbox of srcFile as a string "xmin ymin xmax ymax" [string]
    """

    from osgeo import gdal
    gdal.UseExceptions()

    src = gdal.Open(srcFile)

    upx, xres, xskew, upy, yskew, yres = src.GetGeoTransform()
    cols = src.RasterXSize
    rows = src.RasterYSize
     
    ulx = upx + 0*xres + 0*xskew
    uly = upy + 0*yskew + 0*yres
     
    llx = upx + 0*xres + rows*xskew
    lly = upy + 0*yskew + rows*yres
     
    lrx = upx + cols*xres + rows*xskew
    lry = upy + cols*yskew + rows*yres
     
    urx = upx + cols*xres + 0*xskew
    ury = upy + cols*yskew + 0*yres

    bbox = f"{llx} {lly} {urx} {ury}" #xmin (llx) ymin (lly) xmax (urx) ymax (ury)

    return bbox


def extract_Vectorbbox(srcFile):

    """
    srcFile: path/to/source/vector/file.gpkg [string] or any fiona-driver compatible format

    output : bbox of srcFile as a string "xmin ymin xmax ymax" [string]
    """

    import geopandas as gpd

    gdf = gpd.read_file(srcFile)
    bbox = f" {gdf.total_bounds[0]} {gdf.total_bounds[1]} {gdf.total_bounds[2]} {gdf.total_bounds[3]}"

    del gdf

    return bbox


def request_locations_hubeau(bbox,dstFile,operating):

    """
    Makes a request to https://hubeau.eaufrance.fr/api/v1/hydrometrie/referentiel/stations? and returns a geoDataFrame with station codes
    with colomuns ['code_station','date_ouverture_station','date_fermeture_station','geometry']

    bbox: coordinates as a string "xmin ymin xmax ymax" to define the request's spatial extent [string]
    dstFile: /path/to/destination/layer.gpkg containing the geoDataFrame projected in EPSG:4326 [string]
    operating: geoDataFrame will be filtered to only still operating stations if True, or will keep all returned stations if False [python object]
    """

    #Import libraries
    import geopandas as gpd
    import pandas as pd
    import requests
    import json
    import shapely

    #Split coordinates string 
    l = bbox.split()
    xmin = l[0]
    ymin = l[1]
    xmax = l[2]
    ymax = l[3]
    
    print("Request to hubeau.eaufrance.fr")
    size = 10000 #maximum depth of the json response from hubeau.eaufrance.fr 
    url = f"https://hubeau.eaufrance.fr/api/v1/hydrometrie/referentiel/stations?size={size}&pretty&bbox={xmin},{ymin},{xmax},{ymax}"
    r = requests.get(url, allow_redirects=True)
    
    #Convert data from json to dataframe
    
    data = json.loads(r.content)
    df_stations = pd.json_normalize(data['data'])
    df_stations['longitude_station'] = df_stations['longitude_station'].astype("string")
    df_stations['latitude_station'] = df_stations['latitude_station'].astype("string")
    df_stations['coordonnees']="POINT ("+df_stations['longitude_station']+" "+df_stations['latitude_station']+")"
    geometry = df_stations['coordonnees'].map(shapely.wkt.loads)
    df_stations.drop('coordonnees', axis=1, inplace=True)
    gdf_stations = gpd.GeoDataFrame(df_stations, crs="EPSG:4326", geometry=geometry)
    
    #Filter dataframe to only specific columns and set integer index
    champs = ['code_station','date_ouverture_station','date_fermeture_station','geometry']
    gdf_stations1 = gdf_stations[champs]

    #Filter dataframe according to the value of operating parameter and set integer index
    if operating is True:
        m = gdf_stations1['date_fermeture_station'].isnull()
        gdf_stations2 = gdf_stations1.loc[m]
        m = gdf_stations2['date_ouverture_station'].notna()
        gdf_stations3 = gdf_stations2.loc[m]
        gdf_stations3.set_index(pd.Index([i for i in range(len(gdf_stations3))]),inplace=True)
    else:
        gdf_stations3 = gdf_stations1
        gdf_stations3.set_index(pd.Index([i for i in range(len(gdf_stations3))]),inplace=True)
    
    #Write dataframe to disk as .gpkg
    gdf_stations3.to_file(dstFile)

    return


def requestBackend_observations_hubeau(station,start,end):
    
    """
    stations: code_station [string]
    start: YYYY-MM-DD [string]
    end: YYYY-MM-DD [string]
    """
    
    import numpy as np
    import datetime
    import requests
    import pandas as pd

    #Création d'un df vide pour la concaténation des df de chaque station
    champs = ['code_station','date_obs_elab','resultat_obs_elab','grandeur_hydro_elab','libelle_statut']
    df_hydro = pd.DataFrame(columns=champs)

    #Paramètres statiques de la requête GET 
    size = 10000
    grandeur = 'QmJ' #Note : la requête avec QmM renvoie un QmJ... 

    #paramètres dynamiques

    elem = station

    ##Date d'ouverture
    ouv_string = start

    ##Date de fermeture
    ferm_string = end

    ##Correction de ouv_string dans le cas où la taille de la requête dépasse la taille max de la réponse

    ouv_datetime = datetime.datetime.strptime(ouv_string, '%Y-%m-%d')
    ferm_datetime = datetime.datetime.strptime(ferm_string, '%Y-%m-%d')
    delta = ferm_datetime - ouv_datetime
    if delta.days > size:
        ouv_string = str(ferm_datetime - datetime.timedelta(days=size + 1)) #+1 par sécurité
        ouv_string = ouv_string[:10]

    #Enregistrement des QmJ dans un dataframe pour chaque station et concaténation dans un df global 

    url = 'https://hubeau.eaufrance.fr/api/v1/hydrometrie/obs_elab?\
size={size}&pretty&\
code_entite={elem}&\
grandeur_hydro_elab={grandeur}&\
date_debut_obs_elab={deb}&\
date_fin_obs_elab={fin}'.format(elem=elem,size=size,grandeur=grandeur,deb=ouv_string,fin=ferm_string)

    print(url)

    r = requests.get(url, allow_redirects=True)

    if  r.status_code != 503:

        import json
        data = json.loads(r.content)
        df_reponse = pd.json_normalize(data['data'])

        if df_reponse.empty is True: #cas où la réponse json est nulle

            print("Réponse nulle de l'url : "+url)

        else:

            #Indexation du df_reponse  
            df_reponse = df_reponse[champs]
            indexing = [x for x in range(len(df_hydro),len(df_hydro)+len(df_reponse))]
            df_reponse.set_index(pd.Index(indexing),inplace=True)

            #Fusion
            frames = [df_hydro, df_reponse]
            df_hydro = pd.concat(frames)


    else:
        print('Error '+str(r.status_code)+' for url '+url)
        pass
    
    return df_hydro



def requestBackendV2_observations_hubeau(stationCode,start,end):

    """
    stations: code_station [string]
    start: YYYY-MM-DD [string]
    end: YYYY-MM-DD [string]
    """

    #Import libraries 
    import datetime
    import geopandas as gpd
    import pandas as pd
    import requests
    import json
    import numpy as np

    #Request observations
    
    grandeur = "QmJ"
    size = 10000
    
    url = f"https://hubeau.eaufrance.fr/api/v1/hydrometrie/obs_elab?\
size={str(size)}&pretty&\
code_entite={str(stationCode)}&\
grandeur_hydro_elab={grandeur}&\
date_debut_obs_elab={str(start)}&\
date_fin_obs_elab={str(end)}"
    
    r = requests.get(url, allow_redirects=True)
        
    if  r.status_code != 503:
    
        data = json.loads(r.content)
        df_response = pd.json_normalize(data['data'])

        #Clear response dataframe and store it if it is not empty

        if df_response.empty is not True:

            #Select specific fields
            fields = ['code_station','date_obs_elab','resultat_obs_elab','grandeur_hydro_elab','libelle_statut']
            dfc = df_response[fields]
            del df_response
        
        else:
            print(f"null response from url {url}")
            
            frames = {'code_station':[str(stationCode)],
                      'date_obs_elab':[np.float32(np.nan)],
                      'resultat_obs_elab':[np.float32(np.nan)],
                      'grandeur_hydro_elab':[np.float32(np.nan)],
                      'libelle_statut':[np.float32(np.nan)]}
            dfc = pd.DataFrame(frames)
    
    else:
        print(f"error 503 from url {url}")
        
        frames = {'code_station':[str(stationCode)],
                      'date_obs_elab':[np.float32(np.nan)],
                      'resultat_obs_elab':[np.float32(np.nan)],
                      'grandeur_hydro_elab':[np.float32(np.nan)],
                      'libelle_statut':[np.float32(np.nan)]}
        dfc = pd.DataFrame(frames)

    
    return dfc


def requestFrontendV2_observations_hubeau(srcFile,dstFile):

    """
    Makes a request to https://hubeau.eaufrance.fr/api/v1/hydrometrie/obs_elab? and returns a geoDataFrame with station observations (Mean Daily Flows)
    with colomuns ['code_station','date_obs_elab','resultat_obs_elab','geometry']
    Sations observations are extracted from each station's openning date to present date

    srcFile: /path/to/source/layer.gpkg containing the geoDataFrame of station codes #index must be integers [string]
    dstFile: /path/to/destination/layer.gpkg [string]
    """

    #Import libraries 
    import numpy as np
    import datetime
    import geopandas as gpd
    import pandas as pd
    import requests
    import json

    #Read station location file
    gdf = gpd.read_file(srcFile)
    gdf.drop_duplicates(subset='code_station',inplace=True)
    gdf.reset_index(drop=True,inplace=True)
    
    #Itération sur les stations

    stations_observ = []
    
    for s in range(len(gdf)):
        
        stas = gdf.loc[s,'code_station']
        geom = gdf.loc[s,'geometry']
        print("Station",stas)
        
        #Calcul du nombre de requêtages nécessaires selon la date d'ouverture de la station
        
        Nmax = 10000 #profondeur max de la requête
        
        mask = gdf['code_station'].str.match(str(stas))
        ouv = list(gdf.loc[mask,"date_ouverture_station"])
        ouv_string = ouv[0][:10]
        ouv_datetime = datetime.datetime.strptime(ouv_string, '%Y-%m-%d')
        
        ferm = str(datetime.datetime.now())
        ferm_string = ferm[:10]
        ferm_datetime = datetime.datetime.strptime(ferm_string, '%Y-%m-%d')
    
        delta = ferm_datetime - ouv_datetime
        nb_iter = np.trunc(delta.days/Nmax)+1 #partie entière du nombre d'itérations
        
        #Requêtages et concaténations successives
        
        print("closing_datetime",ferm_datetime)
        print("openning_datetime",ouv_datetime)
        
        for i in range(int(nb_iter)):
            
            save_deb = 0
            fin = 0
            
            incr = datetime.timedelta(days=(i+1)*Nmax)
            
            if ferm_datetime-incr <= ouv_datetime:
                
                tmp = str(ouv_datetime)
                deb = tmp[:10]
                print("deb_obs",deb)
                
                tmp = str(ferm_datetime - datetime.timedelta(days=i*Nmax))
                fin = tmp[:10]
                print("fin_obs",fin)
                
                df_observ = requestBackendV2_observations_hubeau(stas,deb,fin)
                #Add geometry column
                df_observ.loc[:,'geometry'] = geom
                stations_observ.append(df_observ)
                del df_observ
            
            else:
                
                tmp = str(ferm_datetime - datetime.timedelta(days=(i+1)*Nmax-1))
                deb = tmp[:10]
                print("deb_obs",deb)
                
                tmp = str(ferm_datetime - datetime.timedelta(days=i*Nmax))
                fin = tmp[:10]
                print("fin_obs",fin)
                
                df_observ = requestBackendV2_observations_hubeau(stas,deb,fin)
                #Add geometry column
                df_observ.loc[:,'geometry'] = geom
                stations_observ.append(df_observ)
                del df_observ
        

    #Concatenate every dataframes of all stations and convert to geodataframe
    df_stations = pd.concat(stations_observ)
    gdf_stations = gpd.GeoDataFrame(df_stations, crs="EPSG:4326")
    gdf_stations.set_geometry('geometry')

    #Write to disk 
    gdf_stations.to_file(dstFile)

    return



def requestFrontendV2_observations_hubeau_station(srcFile,dstFile,stationCode):

    """
    Makes a request to https://hubeau.eaufrance.fr/api/v1/hydrometrie/obs_elab? and returns a geoDataFrame with station observations (Mean Daily Flows)
    with colomuns ['code_station','date_obs_elab','resultat_obs_elab','geometry']
    Sations observations are extracted from each station's openning date to present date

    srcFile: /path/to/source/layer.gpkg containing the geoDataFrame of station codes #index must be integers [string]
    dstFile: /path/to/destination/layer.gpkg [string]
    """

    #Import libraries 
    import datetime
    import geopandas as gpd
    import pandas as pd
    import requests
    import json

    #Read station location file
    
    gdf = gpd.read_file(srcFile)
    gdf.drop_duplicates(subset='code_station',inplace=True)
    gdf.set_index('code_station',inplace=True)

    #Create list of days over 100 years to range over
    start = datetime.datetime.now()
    timeRange = 365*100 #days
    incDtime_list = [start - datetime.timedelta(days=d) for d in range(timeRange)]
    days_list = [incDtime_list[i].strftime('%Y-%m-%d') for i in range(len(incDtime_list))] #datetime format YYYY-MM-DD

    #Request observations for each station (time range is maximum for each station)

    grandeur = "QmJ"
    size = 10
    geom = gdf.loc[f"{str(stationCode)}",'geometry']

    observ = []

    for date in days_list:
        
        #Check if current date is greater than the station's openning date
        
        current_date = datetime.datetime.strptime(date, '%Y-%m-%d')
        tmp = gdf.loc[f"{str(stationCode)}",'date_ouverture_station']
        openning_date = datetime.datetime.strptime(tmp[:-10], '%Y-%m-%d')
        
        if current_date >= openning_date:
            
            print("date",str(date))

            url = f"https://hubeau.eaufrance.fr/api/v1/hydrometrie/obs_elab?\
size={str(size)}&pretty&\
code_entite={str(stationCode)}&\
grandeur_hydro_elab={grandeur}&\
date_debut_obs_elab={str(date)}&\
date_fin_obs_elab={str(date)}"
        
            r = requests.get(url, allow_redirects=True)
            
            if  r.status_code != 503:
            
                data = json.loads(r.content)
                df_response = pd.json_normalize(data['data'])
    
                #Clear response dataframe and store it if it is not empty
    
                if df_response.empty is not True:
    
                    #Select specific fields
                    fields = ['code_station','date_obs_elab','resultat_obs_elab','grandeur_hydro_elab','libelle_statut']
                    dfc = df_response[fields]
                    del df_response
                    #Only retrieve first row 
                    df1 = dfc.iloc[:1,:]
                    #Append to list to further concatenate
                    observ.append(df1)
                    del df1
                
                else:
                    pass
            
            else:
                print("error 503")
   
        else:
            pass


    df_observ = pd.concat(observ)
    #Add geometry column
    df_observ.loc[:,'geometry'] = geom
    #Convert to geodataframe
    gdf_observ = gpd.GeoDataFrame(df_observ, crs="EPSG:4326")
    gdf_observ.set_geometry('geometry')
    del df_observ
    #Write to disk
    gdf_observ.to_file(dstFile)
    
    return



def compute_MeanMonthlyFlow(stationCode,stationsLayer,dstLayer,period,generate_plot):
    
    """
    stationCode: code of the station to be analyzed [string]
    stationsLayer: source layer that is the geodataframe with stations observations #must be a gdf already loaded [geopandas object]
    where columns must be labelled as follow ['code_station','date_obs_elab','resultat_obs_elab','grandeur_hydro_elab','libelle_statut','geometry']
    period: time window to perform the analysis upon given as a list [start,end] with date format YYYY [list]
    generate_plot: if set to True, the function also creates a plot of MMF along the period [python object]
    
    dstLayer: /path/to/destination/layer.gpkg that is a geodataframe with columns = ['code_station','MMFmu_#month_#period','MMFsigma_#month_#period','geometry']
    """
    
    import re
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import geopandas as gpd
    import datetime
    
    dfc_hydro = stationsLayer.copy()
    
    #Extraction des relevés hydro pour la station
    dfc_hydro.code_station = dfc_hydro.code_station.astype("string")
    dfc_cut = dfc_hydro[dfc_hydro.code_station.str.match(str(stationCode))]
    dfc_cut.dropna(subset='resultat_obs_elab',axis=0,inplace=True)

    if dfc_cut.empty is not True:
    
        #Grouper par mois et calculer la moyenne
        
        #Convertir la colonne date en objet datetime
        serie = dfc_cut.date_obs_elab.astype('datetime64[ns]')
        df = pd.DataFrame(serie,index=dfc_cut.index)
        dfc_cut = dfc_cut[['code_station','resultat_obs_elab','grandeur_hydro_elab','libelle_statut','geometry']]
        dfm = pd.merge(df, dfc_cut, left_index=True, right_index=True)
        dfm.reset_index(drop=True,inplace=True)
        
        #Vérification de la plage temporelle appelée
        deb = datetime.datetime.strptime(str(period[0]), '%Y')
        fin = datetime.datetime.strptime(str(period[1]), '%Y')  
        
        
        if dfm.date_obs_elab.min() <= deb:
            
            #Restriction du df aux années appelées
            dfm_cut = dfm.loc[(dfm.date_obs_elab >= deb) & (dfm.date_obs_elab <= fin)]
    
            #Réindexation par date
            dfm_cut.set_index('date_obs_elab',inplace=True)
    
            #Grouper par mois avec la moyenne comme agrégateur
            dfm_grouped = dfm_cut.groupby(pd.Grouper(freq="M"))['resultat_obs_elab'].mean()
    
            #Génération des (Yi), du vecteur sigma_Y et du vecteur mu_Y
    
            #Préparation du df pour le regex sur les dates
            df2 = pd.DataFrame(dfm_grouped)
            df2.reset_index(inplace = True)
            df2['date_obs_elab'] = df2['date_obs_elab'].astype("string")
    
            mu_Y = []
            sigma_Y = []
            
            for i in range(12):
    
                #Grouper par mois en créant un df par mois : chaque df correspond à un Yi de la suite (Yi)1<i<12
                if (i+1) <= 9:
                    regex_pattern = r"(\d+)-0{m}-(\d+)".format(m=i+1)
                    p = re.compile(regex_pattern)
                    f = np.vectorize(lambda x: pd.notna(x) and bool(p.search(x)))
                    df_month = df2[f(df2['date_obs_elab'])]
                else:
                    regex_pattern = r"(\d+)-{m}-(\d+)".format(m=i+1)
                    p = re.compile(regex_pattern)
                    f = np.vectorize(lambda x: pd.notna(x) and bool(p.search(x)))
                    df_month = df2[f(df2['date_obs_elab'])]
                
                #Générer les vecteurs de barres d'erreur et de moyennes 
                mu = df_month.resultat_obs_elab.mean()
                sigma = df_month.resultat_obs_elab.std(skipna=True, ddof=1)
                mu_Y.append(mu)
                sigma_Y.append(sigma)
                
                df_month = df_month.iloc[0:0] #del df_month content
          
            #Gather monthly data in a dataframe and save it to disk as .gpkg
           
            dic = {'code_station':[stationCode],
            	f"MMFmu_month1_{period[0]}{period[1]}":[mu_Y[0]],
            	f"MMFmu_month2_{period[0]}{period[1]}":[mu_Y[1]],
            	f"MMFmu_month3_{period[0]}{period[1]}":[mu_Y[2]],
            	f"MMFmu_month4_{period[0]}{period[1]}":[mu_Y[3]],
            	f"MMFmu_month5_{period[0]}{period[1]}":[mu_Y[4]],
            	f"MMFmu_month6_{period[0]}{period[1]}":[mu_Y[5]],
            	f"MMFmu_month7_{period[0]}{period[1]}":[mu_Y[6]],
            	f"MMFmu_month8_{period[0]}{period[1]}":[mu_Y[7]],
            	f"MMFmu_month9_{period[0]}{period[1]}":[mu_Y[8]],
            	f"MMFmu_month10_{period[0]}{period[1]}":[mu_Y[9]],
            	f"MMFmu_month11_{period[0]}{period[1]}":[mu_Y[10]],
            	f"MMFmu_month12_{period[0]}{period[1]}":[mu_Y[11]],
            	f"MMFsigma_month1_{period[0]}{period[1]}":[sigma_Y[0]],
            	f"MMFsigma_month2_{period[0]}{period[1]}":[sigma_Y[1]],
            	f"MMFsigma_month3_{period[0]}{period[1]}":[sigma_Y[2]],
            	f"MMFsigma_month4_{period[0]}{period[1]}":[sigma_Y[3]],
            	f"MMFsigma_month5_{period[0]}{period[1]}":[sigma_Y[4]],
            	f"MMFsigma_month6_{period[0]}{period[1]}":[sigma_Y[5]],
            	f"MMFsigma_month7_{period[0]}{period[1]}":[sigma_Y[6]],
            	f"MMFsigma_month8_{period[0]}{period[1]}":[sigma_Y[7]],
            	f"MMFsigma_month9_{period[0]}{period[1]}":[sigma_Y[8]],
            	f"MMFsigma_month10_{period[0]}{period[1]}":[sigma_Y[9]],
            	f"MMFsigma_month11_{period[0]}{period[1]}":[sigma_Y[10]],
            	f"MMFsigma_month12_{period[0]}{period[1]}":[sigma_Y[11]],
            	'geometry':[dfm.loc[0,'geometry']]
            	}
            	
            dfg = pd.DataFrame(dic)
            final_gdf = gpd.GeoDataFrame(dfg, crs="EPSG:4326")
            final_gdf.set_geometry('geometry',inplace=True)
            final_gdf.to_file(dstLayer)
            
            
            #Draw hydrogram if specified by generate_plot=True
            if generate_plot is True:
                x_axis = [x+1 for x in range(12)]
                #plt.plot(x_axis, mu_Y, 'k') #plot(x,y,format,x,y2,format2,...,x,yN,formatN)
                plt.errorbar(x_axis, mu_Y, yerr = sigma_Y, fmt ='o')
                #plt.axis([1, 12, np.min(mu_Y), 20]) #graph bbox xmin, xmax, ymin, ymax
                plt.xticks(x_axis)
                plt.ylabel('QmM moyen sur la période [l.s-1]',fontsize=9)
                plt.xlabel('mois',fontsize=9)
                plt.title('Hydrogramme mensuel moyen de la station {stas} sur la période {span}'.
                          format(stas=stationCode,span=period),fontsize=9)
                plt.savefig('./debits_mensuels_moyens/dmm_{stas}.png'.format(stas=stationCode),bbox_inches='tight')
                plt.close() #sinon à chaque appel de la fonction la figure est dessinée sur le même graphe
            else:
                pass
            
        else:
            
            print(f"Station {str(stationCode)} does not cover the range {str(period)}")

    else:
        pass

    try:
        final_gdf
    except:
        pass
    else:
        return final_gdf




def extract_Cellsbbox(srcFile):

    """
    srcFile: path/to/source/raster/file.tif [string]

    output : list of coordinates "xmin ymin xmax ymax" for each cell in srcFile [list[string]]
    """
    
    from osgeo import gdal
    gdal.UseExceptions()
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    a = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    
    h,w = a.shape
    
    coords = []
    
    for y in range(h): 

        if y == 0: #yres is negative and y axis goes x-wise
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
            ymin = uly + y * yres + yres 
        
        for x in range(w):

            if x == 0:
                xmin = ulx 
                xmax = ulx + xres
            else:
                xmin = ulx + x * xres 
                xmax = ulx + x * xres + xres

            bbox = f"{xmin} {ymin} {xmax} {ymax}"
            coords.append(bbox)
            if (xmin >= xmax) | (ymin >= ymax):
                print("Error (x,y)",(x,y))
    
    return coords


def extract_cellsValues(srcFile,*dropna):
    
    """
    srcFile: path/to/source/raster/file.tif [string]
    dropna [optional]: if set to True, the returned dict will not include cells with NaN values [Python object]
    output : python dictionary with keys 'coordinates', 'values', 'id' i.e. 'coordinates':["xmin ymin xmax ymax", ...], 'value':[...], 'id':[(y,x),...] for each cell in srcFile [dict]
    """
    
    from osgeo import gdal
    gdal.UseExceptions()
    import numpy as np
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    a = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    
    h,w = a.shape
    
    coords = []

    #Create empty dictionary to further store results
    
    frames = {'coordinates':[],
              'values':[],
             'id':[]}
    
    for y in range(h): 

        if y == 0: #yres is negative and y axis goes x-wise
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
            ymin = uly + y * yres + yres 
        
        for x in range(w):

            if x == 0:
                xmin = ulx 
                xmax = ulx + xres
            else:
                xmin = ulx + x * xres 
                xmax = ulx + x * xres + xres

            if (xmin >= xmax) | (ymin >= ymax):
                print("Error (x,y)",(x,y))
            else:
                pass

            if dropna is True:
            
                if np.float32(a[y,x]) == np.float32(np.nan):
                    pass
                else:
                    frames['coordinates'].append(f"{xmin} {ymin} {xmax} {ymax}")
                    frames['values'].append(np.float32(a[y,x])) #numpy default float format is float64 which is not compatible with pandas.df float32 format (for further uses)
                    frames['id'].append((y,x))
            
            else:
                frames['coordinates'].append(f"{xmin} {ymin} {xmax} {ymax}")
                frames['values'].append(np.float32(a[y,x]))
                frames['id'].append((y,x))
    
    return frames


def convert_to_pcraster(srcFile,dstFile,epsgCode,cloneFile):

    """
    Convert continuous .tif raster to a scalar .map raster

    maskFile : path/to/mask/raster/file.tif used to retrieve a geoTransform and set proper bbox to the PCRatser file
    """

    import os
    from osgeo import gdal, gdalconst
    gdal.UseExceptions()

    #Read maskfile to get geotransfrom info
    mask = gdal.Open(cloneFile)
    upx, xres, xskew, upy, yskew, yres = mask.GetGeoTransform()
    cols = mask.RasterXSize
    rows = mask.RasterYSize
     
    ulx = upx + 0*xres + 0*xskew
    uly = upy + 0*yskew + 0*yres
     
    llx = upx + 0*xres + rows*xskew
    lly = upy + 0*yskew + rows*yres
     
    lrx = upx + cols*xres + rows*xskew
    lry = upy + cols*yskew + rows*yres

    #Set gdal options
    ot = gdalconst.GDT_Float32
    mtdOptions = "VS_SCALAR"
    
    #GDAL Translate
    cmd = f"gdal_translate -a_srs EPSG:{str(epsgCode)} -a_ullr {str(ulx)} {str(uly)} {str(lrx)} {str(lry)} -ot Float32 -of PCRaster {str(srcFile)} {str(dstFile)}"
    os.system(cmd)
    #Note: still with a proper geoTransform options setting, .tif and .map rasters do not exactly overlap...
    
    #Properly close the datasets to flush to disk
    del mask

    return

def convert_to_geotiff(srcFile,dstFile,epsgCode):

    """
    Convert scalar .map raster to a contiuous .tif raster
    """

    from osgeo import gdal
    gdal.UseExceptions()
    import os

    #GDAL Translate
    cmd = f"gdal_translate -a_srs EPSG:{str(epsgCode)} -of GTiff {str(srcFile)} {str(dstFile)}"
    os.system(cmd)

    return


def convert_to_csv(srcFile,dstFile):

    """
    Exports vector layer format .gpkg toformat .csv with ogr2ogr
    """

    from osgeo import ogr
    import os

    #Delete dstFile in the case that it already exists
    if os.path.isfile(dstFile) is True:
        os.remove(dstFile)
    
    cmd = 'ogr2ogr -f "CSV" -lco GEOMETRY=AS_XY {dstFile} {srcFile}'.format(dstFile=dstFile,srcFile=srcFile)
    os.system(cmd)

    return

def convert_to_vrt(srcFile,layerName,zName,xName,yName):

    """

    srcFile: /path/to/csv/file.csv 
    zField: column name in the source .csv file to be used as the z value
    xName: column name in the source .csv file to be used as the X-axis coordinates
    yName: column name in the source .csv file to be used as the Y-axis coordinates
    layerName: given the path /path/to/csv/file.csv, layerName = "file"

    output: .vrt file with the same name as layerName and same path
    """

    from osgeo import ogr
    import os

    #One key <OGRVRTLayer> for one field in the csv file (one field at a time)
    headers = ['<OGRVRTDataSource>\n',
               '    <OGRVRTLayer name="{layerName}">\n'.format(layerName=layerName),
               f"        <SrcDataSource>{srcFile}</SrcDataSource>\n",
               '        <GeometryType>wkbPoint</GeometryType>\n',
               '        <GeometryField encoding="PointFromColumns" x="{xName}" y="{yName}" z="{zName}"/>\n'.format(xName=xName,yName=yName,zName=zName),
               '    </OGRVRTLayer>\n',
               '</OGRVRTDataSource>']
    
    vrtFile = f"{srcFile[:-4]}.vrt"
    f = open(vrtFile,"w+")
    f.writelines(headers)
    f.close()

    #Check and display whether everything is OK
    os.system(f"ogrinfo -if VRT {vrtFile}")

    return

def interpolate(scatterFile,layerName,maskFile,dstEPSG,algorithm,parameters):

    """
    scatterFile: /path/to/vrt/scatter/file.vrt
    layerName: given the path /path/to/vrt/scatter/file.vrt, layerName = "file" 
    maskFile: /path/to/raster/file.tif from which geotransform properties will be retrieved and applied to dstFile
    algorithm: name of interpolation method e.g. "invdist" (see https://gdal.org/programs/gdal_grid.html#interpolation-algorithms)
    parameters: list of parameters starting with ":" and separated with ":" e.g. ":power=2:smoothing=0.05". If no parameter needed, enter empty string "" 
    
    output: interpolated raster in .tif format with same path as scatterFile but ending with "_gridded_{algorithm}_{parameters}.tif"
    """
    
    from osgeo import gdal
    gdal.UseExceptions()
    import os
    
    #Read maskfile to get geotransfrom info
    mask = gdal.Open(maskFile)
    upx, xres, xskew, upy, yskew, yres = mask.GetGeoTransform()
    cols = mask.RasterXSize
    rows = mask.RasterYSize
    
    #Setting parameters for output grid cells
    dstFile = f"{scatterFile[:-4]}_gridded_{algorithm}_{parameters}.tif"
    txe = f"{upx} {upx+cols*xres}"#xmin xmax 
    tye = f"{upy+rows*yres} {upy}" #ymin ymax (yres is <0)
    size = f"{cols} {rows}" #xsize ysize
    of = "GTiff"
    ot = "Float64"
    epsgCode = 4326
    
    #execute gdal_grid
    cmd = f"gdal_grid -a {algorithm}{parameters} -a_srs EPSG:{str(dstEPSG)} -txe {txe} -tye {tye} -outsize {size} -of {of} -ot {ot} -l {layerName} {scatterFile} {dstFile}"
    print(cmd)
    os.system(cmd)
    #g = gdal.Grid(f"/home/q.dassibat/python_proj/mamba/jupyter_modules/data/ESB/ModelOutputResampling/outlet_flows_band1_cutoff{cutoff}.tif",scatterFile,outputSRS="EPSG:4326")
    
    #Flush memory 
    mask = None

    return



def create_flowdirection(srcFile, dstFile, cloneFile, outflowdepth, corevolume, corearea, catchmentprecipitation):

    """
    Remove pits from a DEM raster (prior conversion to PCRaster format is required) through generating a FlowDirection map
    Parameters to remove pits (when the cell under consideration is higher than the 4 values, it is considered a pit and not removed) 
    So to remove all pits set very high value e.g. 1e31
    
    cloneFile: Only the PCRaster file format is supported as input argument
    """ 

    from pcraster import readmap, setclone, lddcreate, report
    import os
    #import importlib
    #importlib.invalidate_caches()

    #Create clone map 
    setclone(cloneFile)

    #Create flow direction map
    dem = readmap(srcFile)
    flowDirection = lddcreate(dem,outflowdepth,corevolume,corearea,catchmentprecipitation)
    report(flowDirection,dstFile)

    #Close variables
    dem = None
    flowDirection = None

    return

def create_accuflux(flowDirection, material, dstFile, cloneFile):

    """
    Creates accuflux .map based on flowDirection .map and material .map

    cloneFile: Only the PCRaster file format is supported as input argument
    """ 

    from pcraster import setclone, readmap, accuflux, report
    #import gc
    #gc.collect()
    #import importlib
    #importlib.invalidate_caches()

    #Create clone map 
    setclone(cloneFile)

    #Create accuflux map
    flowdir = readmap(flowDirection)
    mat = readmap(material)
    acc = accuflux(flowdir,mat)
    report(acc,dstFile)

    #Close variables
    del flowdir
    del mat
    del acc

    return

def create_material(maskFile,dstFile,epsgCode,materialValue,initialConditionsArray=None):
    
    """
    Creates a raster .tif scalar file with every cell equals to materialValue and with geo properties equals to maskFile
    Usefull to generate a material map, which together with a flow direction map, are used to derive an accuflux map 

    initialConditionsArray [optional] : numpy.array of shape (nb_rows,2) with each element [initial_accuflux,index].
    It must be indexed on a same-shape raster as mskFile
    """

    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()
    gdalDataTypes = {
              "uint8": 1,
              "int8": 1,
              "uint16": 2,
              "int16": 3,
              "uint32": 4,
              "int32": 5,
              "float32": 6,
              "float64": 7,
              "complex64": 10,
              "complex128": 11
            } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read maskfile to get geotransfrom info
    
    r = gdal.Open(maskFile)
    band = r.GetRasterBand(1)
    a = band.ReadAsArray().astype(float)
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    nrows, ncols = a.shape
    geoT = r.GetGeoTransform()

    #Generate numpy array with value = materialValue and properties = maskFile
    
    materialArray = np.full(a.shape,np.float32(materialValue))

    #Apply optional initialConitionsArray

    if initialConditionsArray is None:

        pass

    else:

        conditionsArray = np.array(initialConditionsArray)
        
        #Generate the correspondance matrix between pixel index in raster maskFile and array
        
        index_matrix = np.empty([nrows,ncols])
        index_matrix[:] = np.float32(np.nan)
        index = 0
    
        for y in range(nrows): 
            if y == 0: #yres is negative and y axis goes x-wise
                ymax = uly 
                ymin = uly + yres
            else:
                ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
                ymin = uly + y * yres + yres 
            for x in range(ncols):
                if x == 0:
                    xmin = ulx 
                    xmax = ulx + xres
                else:
                    xmin = ulx + x * xres 
                    xmax = ulx + x * xres + xres
                if (xmin >= xmax) | (ymin >= ymax):
                    print("Error (x,y)",(x,y))
                else:
                    pass
                
                index_matrix[y,x] = np.int32(index)
                index += 1

        #Modify materialArray with initialConditionsArray based on index_matrix

        index = 0
        h,w = conditionsArray.shape

        #Range over elements in materialArray
        for y in range(nrows):
            
            for x in range(ncols):

                #Find matching index in initialConditionsArray if any and change corresponding materialValue accordingly
                for row in range(h):
                    
                    i = conditionsArray[row,1]
                    
                    if np.int32(i) == np.int32(index):

                        initialValue = np.float32(conditionsArray[row,0])
                        materialArray[y,x] = initialValue

                    else:
                        pass
                        
                index += 1

    #Export array as dstFile .tif

    gdalType = gdalDataTypes[materialArray.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, ncols, nrows, 1, gdalType)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(materialArray)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    return


def create_raster(array,maskFile,dstFile,epsgCode):
    
    """
    Creates a raster .tif scalar file from a numpy array with geo properties equals to maskFile 
    """

    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    #Read maskfile to get geotransfrom info
    
    r = gdal.Open(maskFile)
    band = r.GetRasterBand(1)
    a = band.ReadAsArray().astype(float)

    nrows, ncols = a.shape
    geoT = r.GetGeoTransform()

    #Export array as dstFile .tif

    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, ncols, nrows, 1, gdal.GDT_Float32)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(array)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    return

def points_to_raster(gdfLayer,valuesColumn,maskFile,dstFile,epsgCode):

    """
    gdfLayer: geodataframe layer (already loaded with gpd), must be POINT geometry and geom column name "geometry" and SAME DIMENSION as maskFile raster [string]
    valuesColumn: "column_name" in gdfLayer containing values to write in the output raster [string]
    maskFile: /path/to /mask/raster/file.tif from which geo properties will be applied to dstFile [string]
    dstFile: /path/to/output/raster.tif [string]
    """

    import rioxarray
    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    #Convert geodataframe to rioxarray.xarray and then to numpy.array
    gdfLayer['x']=gdfLayer['geometry'].x
    gdfLayer['y']=gdfLayer['geometry'].y
    xarr = gdfLayer.set_index(["y", "x"])[f"{str(valuesColumn)}"].to_xarray()
    arr = xarr.values
    farr = np.flip(arr,0) #flip array indexing vertically

    #Read maskfile to get geotransfrom info
    r = gdal.Open(maskFile)
    band = r.GetRasterBand(1)
    a = band.ReadAsArray().astype(float)
    nrows, ncols = a.shape
    geoT = r.GetGeoTransform()

    #Export array as dstFile .tif
    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, ncols, nrows, 1, gdal.GDT_Float32)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(farr)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    return
    


def getId_subcatchments(outletsLayer,idLabel):

    """
    outletsLayer: /path/to/vector/layer.gpkg where outlets identifiers are contained [string]
    idLabel: colomn name indexing the outlets identifiers, e.g. 'uid_outlets' [string]
    Output: list of tuples (idx,id) with corresponding index and outlet id, where idx is a [string] format "idx" and id is a [string] format "(y,x)" 
    """
    
    import geopandas as gpd
    import os
    from pcraster import readmap, subcatchment, report
    wd = os.getcwd()
    
    
    #Convert .gpkg outletsLayer to .txt file with structure: x_coord y_coord id
    
    gdf = gpd.read_file(outletsLayer)
    coords_wkt = list(gdf['geometry'])
    coords_string = [str(x) for x in coords_wkt]
    coords_export = []
    identifiers = []
    idx = 0
    for elem in coords_string:
        tmp = elem[6:]
        coords_split = tmp.split(" ")
        xcoord = coords_split[0][1:]
        ycoord = coords_split[1][:-1]
        ident = gdf[f"{idLabel}"].iloc[idx]
        identifiers.append((str(idx+1),str(ident))) #used to save the actual cell id as PCRaster cannot deal with string datatype
        if idx != len(coords_string)-1:
            coords_export.append(f"{str(xcoord)}"+" "+f"{str(ycoord)}"+" "+f"{str(idx+1)}"+"\n")
        else:
            coords_export.append(f"{str(xcoord)}"+" "+f"{str(ycoord)}"+" "+f"{str(idx+1)}")
        idx += 1

    return identifiers


def create_subcatchments(outletsLayer,flowdirectionRaster,dstRaster,cloneMap):
    
    """
    outletsLayer: /path/to/point/vector/layer.gpkg or any compatible format with geopandas drivers, must be point geometry [string]
    flowdirectionRaster: /path/to/flowdirection/raster.map [string]
    cloneMap: /path/to/pcraster/clone.map [string]
    
    dstRaster: /path/to/destination/raster.map where subcatchments appear on the same raster with one pixel value for each [string]
    """
    
    import geopandas as gpd
    import os
    from pcraster import readmap, subcatchment, report
    wd = os.getcwd()
    #import gc
    #gc.collect()
    
    #Convert .gpkg outletsLayer to .txt file with structure: x_coord y_coord id
    
    gdf = gpd.read_file(outletsLayer)
    coords_wkt = list(gdf['geometry'])
    coords_string = [str(x) for x in coords_wkt]
    coords_export = []
    idx = 0
    for elem in coords_string:
        tmp = elem[6:]
        coords_split = tmp.split(" ")
        xcoord = coords_split[0][1:]
        ycoord = coords_split[1][:-1]
        if idx != len(coords_string)-1:
            coords_export.append(f"{str(xcoord)}"+" "+f"{str(ycoord)}"+" "+f"{str(idx+1)}"+"\n")
        else:
            coords_export.append(f"{str(xcoord)}"+" "+f"{str(ycoord)}"+" "+f"{str(idx+1)}")
        idx += 1
    
    txtFile = f"{wd}/tmp_outlets.txt"
    if os.path.isfile(txtFile) is True:
        os.remove(txtFile)
    else:
        pass
    f = open(txtFile,"w+")
    f.writelines(coords_export)
    f.close()
    
    
    #Convert .txt file to .map PCRaster file
    
    dst = f"{wd}/tmp_outlets_pcraster.map"
    if os.path.isfile(dst) is True:
        os.remove(dst)
    else:
        pass
    cmd = f"col2map --clone {cloneMap} -N {txtFile} {dst}"
    print(cmd)
    os.system(cmd)
    
    #Generate catchments map with FlowDirection map and Outlet map 

    #FlowDirection = readmap(flowdirectionRaster)
    #OutletMap = readmap(dst)
    Catch = subcatchment(flowdirectionRaster,dst)
    report(Catch,f"{dstRaster}")
    
    #Flush memory 
    #os.remove(txtFile)
    #os.remove(dst)
    FlowDirection = None
    OutletMap = None
    Catch = None
    cloneMap = None
    
    
    return

def clip_accuflux_to_subcatchments(accufluxRaster,subcatchmentsRaster,epsgCode,catchmentsList=None):

    """
    accufluxRaster: /path/to/acculfux/raster.map [string]
    subcatchmentsRaster: /path/to/subcatchments/raster.map with 1 band and 1 unique pixel value for each subcatchment [string]
    catchmentsList [optional]: list of catchment ids to be processed e.g. [1,4,53], to avoid processing every catchment in subcatchmentsRaster [list] 
    
    output: rasters with same path as accufluxRaster but with extension "_catch##.tif" [string]
    """

    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()
    gdalDataTypes = {
                  "uint8": 1,
                  "int8": 1,
                  "uint16": 2,
                  "int16": 3,
                  "uint32": 4,
                  "int32": 5,
                  "float32": 6,
                  "float64": 7,
                  "complex64": 10,
                  "complex128": 11
                } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Open rasters
    
    r = gdal.Open(accufluxRaster)
    band = r.GetRasterBand(1)
    accuflux = band.ReadAsArray().astype(float)
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    h,w = accuflux.shape

    r2 = gdal.Open(subcatchmentsRaster)
    band2 = r2.GetRasterBand(1)
    subcatch = band2.ReadAsArray().astype(int)

    #Iterate over each subcatchment and create one s at a time 
    
    if len(list(catchmentsList))==0:
    
        nb_subcatch = np.max(subcatch)
        
        for catch in range(1,nb_subcatch+1): #catchment=0 is a "fake" catchment generated by pcraster.subcatchment(), so forget it
    
            print('Catchment', catch)
    
            #Create an empty array populated with NaN values to further store results
        
            array = np.empty([h,w])
            array[:] = np.float32(np.nan)
        
            #Range over cells and retrieve cells on condition
        
            for y in range(h): 
        
                if y == 0: #yres is negative and y axis goes x-wise
                    ymax = uly 
                    ymin = uly + yres
                else:
                    ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
                    ymin = uly + y * yres + yres 
                
                for x in range(w):
        
                    if x == 0:
                        xmin = ulx 
                        xmax = ulx + xres
                    else:
                        xmin = ulx + x * xres 
                        xmax = ulx + x * xres + xres
        
                    if (xmin >= xmax) | (ymin >= ymax):
                        print("Error (x,y)",(x,y))
                    else:
                        pass
                    
                    if np.int32(subcatch[y,x]) == np.int32(catch):
                        array[y,x] = np.float32(accuflux[y,x])
                    else:
                        array[y,x] = np.float32(np.nan)
        
            #Export array as dstFile .tif
            
            gdalType = gdalDataTypes[array.dtype.name]
            dstFile = f"{accufluxRaster[:-4]}_catch{str(catch)}.tif"
            outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, 6)
            outBand = outDs.GetRasterBand(1)
            outBand.WriteArray(array)
            outDs.SetGeoTransform(geoT)
            srs = osr.SpatialReference()
            srs.ImportFromEPSG(epsgCode)
            outDs.SetProjection(srs.ExportToWkt())
            outDs = None

    else:
        
        for catch in list(catchmentsList): #catchment=0 is a "fake" catchment generated by pcraster.subcatchment(), so forget it
    
            print('Catchment', catch)
    
            #Create an empty array populated with NaN values to further store results
        
            array = np.empty([h,w])
            array[:] = np.float32(np.nan)
        
            #Range over cells and retrieve cells on condition
        
            for y in range(h): 
        
                if y == 0: #yres is negative and y axis goes x-wise
                    ymax = uly 
                    ymin = uly + yres
                else:
                    ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
                    ymin = uly + y * yres + yres 
                
                for x in range(w):
        
                    if x == 0:
                        xmin = ulx 
                        xmax = ulx + xres
                    else:
                        xmin = ulx + x * xres 
                        xmax = ulx + x * xres + xres
        
                    if (xmin >= xmax) | (ymin >= ymax):
                        print("Error (x,y)",(x,y))
                    else:
                        pass
                    
                    if np.int32(subcatch[y,x]) == np.int32(catch):
                        array[y,x] = np.float32(accuflux[y,x])
                    else:
                        array[y,x] = np.float32(np.nan)
        
            #Export array as dstFile .tif
            
            gdalType = gdalDataTypes[array.dtype.name]
            dstFile = f"{accufluxRaster[:-4]}_catch{str(catch)}.tif"
            outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, 6)
            outBand = outDs.GetRasterBand(1)
            outBand.WriteArray(array)
            outDs.SetGeoTransform(geoT)
            srs = osr.SpatialReference()
            srs.ImportFromEPSG(epsgCode)
            outDs.SetProjection(srs.ExportToWkt())
            outDs = None
        
    
    r = None
    r2 = None

    return 


def split_multiband(srcFile,epsgCode):

    """
    Creates a raster .tif scalar file for each band in srcFile. No clip nor reprojection operations are performed.
    srcFile: /path/to/source/raster/file.tif
    dstFile follows the geoTransform propreties of srcFile and has path f"{srcFile[:-4]}_band{#}.tif"
    """

    #Import libraries
    from osgeo import gdal, osr
    gdal.UseExceptions()
    

    #Open srcFile and geoT information    
    src = gdal.Open(srcFile)
    geoT = src.GetGeoTransform()
    ncols = src.RasterXSize
    nrows = src.RasterYSize
    
    #Convert each band in srcFile to np.array and export as raster file
    
    for b in range(src.RasterCount):
        
        band = src.GetRasterBand(b+1)
        array = band.ReadAsArray().astype(float)

        dstFile = f"{srcFile[:-4]}_band{b+1}.tif"
        
        outDs = gdal.GetDriverByName('GTiff').Create(dstFile, ncols, nrows, 1, gdal.GDT_Float32)
        outBand = outDs.GetRasterBand(1)
        outBand.WriteArray(array)
        outDs.SetGeoTransform(geoT)
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(epsgCode)
        outDs.SetProjection(srs.ExportToWkt())
        
        outDs = None
        band = None
        array = None
        dstFile = None

    src = None
    
    return


def split_singleband(srcFile,epsgCode,zRestriction=None):

    """
    Creates a raster .tif scalar file for each pixel value in the single band raster srcFile. Pixel values must be id-like. 
    srcFile: /path/to/source/raster/file.tif
    zRestriction [optional]: list of pixel values to be processed in the srcFile zField [list]
    dstFile follows the geoTransform propreties of srcFile and has path f"{srcFile[:-4]}_PixelValueIs{#}.tif"
    """

    #Import libraries
    
    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    gdalDataTypes = {
                      "uint8": 1,
                      "int8": 1,
                      "uint16": 2,
                      "int16": 3,
                      "uint32": 4,
                      "int32": 5,
                      "float32": 6,
                      "float64": 7,
                      "complex64": 10,
                      "complex128": 11
                    } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read srcFile and extract info
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    src = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    
    h,w = src.shape

    max = band.GetMaximum()

    r = None


    #Iterate over each each pixel value in srcFile and create a separate dstFile for each value

    if zRestriction is None:

        for val in range(int(max)+1):
    
            print(f"Pixel value {str(val)} out of {str(max)}")
        
            #Create an empty array populated with NaN values to further store results
            
            array = np.empty([h,w])
            array[:] = np.float32(np.nan)
        
            #Range over cells and retrieve cells on condition
        
            for y in range(h): 
        
                if y == 0: #yres is negative and y axis goes x-wise
                    ymax = uly 
                    ymin = uly + yres
                else:
                    ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
                    ymin = uly + y * yres + yres 
                
                for x in range(w):
        
                    if x == 0:
                        xmin = ulx 
                        xmax = ulx + xres
                    else:
                        xmin = ulx + x * xres 
                        xmax = ulx + x * xres + xres
        
                    if (xmin >= xmax) | (ymin >= ymax):
                        print("Error (x,y)",(x,y))
                    else:
                        pass
        
                    #Set float values
                    current_pixel = np.float32(src[y,x])
                    target_pixel = np.float32(val)
                
        
                    #Retieve pixel on condition
                    if current_pixel == target_pixel:
                        array[y,x] = current_pixel 
                    else:
                        array[y,x] = np.float32(np.nan)
                
            #Export array as dstFile .tif
    
            dstFile = f"{srcFile[:-4]}_PixelValueIs{str(val)}.tif"
            gdalType = gdalDataTypes[array.dtype.name]
            outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, gdalType)
            outBand = outDs.GetRasterBand(1)
            outBand.WriteArray(array)
            outDs.SetGeoTransform(geoT)
            srs = osr.SpatialReference()
            srs.ImportFromEPSG(epsgCode)
            outDs.SetProjection(srs.ExportToWkt())
            outDs = None

    else:

        for val in list(zRestriction):
    
            print(f"Pixel value {str(val)} in {str(zRestriction)}")
        
            #Create an empty array populated with NaN values to further store results
            
            array = np.empty([h,w])
            array[:] = np.float32(np.nan)
        
            #Range over cells and retrieve cells on condition
        
            for y in range(h): 
        
                if y == 0: #yres is negative and y axis goes x-wise
                    ymax = uly 
                    ymin = uly + yres
                else:
                    ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
                    ymin = uly + y * yres + yres 
                
                for x in range(w):
        
                    if x == 0:
                        xmin = ulx 
                        xmax = ulx + xres
                    else:
                        xmin = ulx + x * xres 
                        xmax = ulx + x * xres + xres
        
                    if (xmin >= xmax) | (ymin >= ymax):
                        print("Error (x,y)",(x,y))
                    else:
                        pass
        
                    #Set float values
                    current_pixel = np.float32(src[y,x])
                    target_pixel = np.float32(val)
                
        
                    #Retieve pixel on condition
                    if current_pixel == target_pixel:
                        array[y,x] = current_pixel 
                    else:
                        array[y,x] = np.float32(np.nan)
                
            #Export array as dstFile .tif
    
            dstFile = f"{srcFile[:-4]}_PixelValueIs{str(val)}.tif"
            gdalType = gdalDataTypes[array.dtype.name]
            outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, gdalType)
            outBand = outDs.GetRasterBand(1)
            outBand.WriteArray(array)
            outDs.SetGeoTransform(geoT)
            srs = osr.SpatialReference()
            srs.ImportFromEPSG(epsgCode)
            outDs.SetProjection(srs.ExportToWkt())
            outDs = None

    return max


def raster_to_polygons(srcFile,dstFile,epsgCode,zName,zRestriction=None):

    """
    Converts raster to polygons with one polygon for a connected region of same pixel value
    This function makes a shadow call to relio.split_singleband() to first create one unique raster for each pixel value in srcFile
    Then it builds temporary polygon vector files and then it puts them together in a geodataframe written to disk as dstFile
    srcFile: /path/to/source/raster/file.tif which must contain a unique pixel value and other pixels = NoData: see function relio.split_singleband() [string]
    dstFile: /path/to/destination/vector/file.gpkg which contains the polygons associated with srcFile [string]
    zName: name of the z value field to be given as a layer in dstFile [string]
    zRestriction [optional]: pass a list of values to lighten domain and to process only these values in zName [list]
    """

    #Import libraries
    
    from osgeo import gdal, osr
    gdal.UseExceptions()
    import geopandas as gpd
    import pandas as pd
    import os

    #Call function relio.split_singleband() to split srcFile raster into multiple rasters with unique pixel value
    max = split_singleband(srcFile,epsgCode,zRestriction) #dstFile = f"{srcFile[:-4]}_PixelValueIs{#}.tif"

    #For each above created raster call gdal_polygonize.py

    vectorList =[]

    if zRestriction is None:
    
        for val in range(1,int(max)+1): #catchment=0 is a "fake" catchment generated by pcraster.subcatchment(), so forget it
    
            rasterFile = f"{srcFile[:-4]}_PixelValueIs{str(val)}.tif"
            tmpVectorFile = f"{srcFile[:-4]}_PixelValueIs{str(val)}.gpkg"
            cmd = 'gdal_polygonize.py {r} -overwrite -b 1 -f "GPKG" {v} OUTPUT {z}'.format(r=rasterFile,v=tmpVectorFile,z=zName)
            os.system(cmd)
            #The above gdal command returns a vector file with multiple entities: one polygon for the envelop, and multiple polygons where region is broken
            #So we remove the envelop (catch_id=0) polygon mnd merge the others 
            tmpVectorLoad = gpd.read_file(tmpVectorFile)
            tmpVectorLoad[f"{zName}"] = tmpVectorLoad[f"{zName}"].astype("string")
            m = tmpVectorLoad[f"{zName}"] == str(val)
            tmpVectorLoadCleared = tmpVectorLoad.loc[m] #returns a gdf without catch_id=0
            union = tmpVectorLoadCleared.unary_union #returns a shapely.geometry
            frames = {'catch_id':[val],
              'geometry' : [union]}
            uniongdf = gpd.GeoDataFrame(frames, crs=f"EPSG:{str(epsgCode)}")
            uniongdf = uniongdf.set_geometry('geometry')
            if os.path.exists(dstFile):
                os.remove(tmpVectorFile)
                uniongdf.to_file(tmpVectorFile)
            else:
                uniongdf.to_file(tmpVectorFile)
            vectorList.append(uniongdf)
            del rasterFile, tmpVectorFile, tmpVectorLoad, tmpVectorLoadCleared, union, uniongdf
    
        #Concatenate each above creater vector layers into a single geodatframe
        df = pd.concat(vectorList)
        gdf = gpd.GeoDataFrame(df, crs=f"EPSG:{str(epsgCode)}")
        gdf = gdf.set_geometry('geometry')
    
        #Write geodataframe to disk
        if os.path.exists(dstFile):
            os.remove(dstFile)
            gdf.to_file(dstFile)
        else:
            gdf.to_file(dstFile)
    

    else:

        for val in list(zRestriction): #catchment=0 is a "fake" catchment generated by pcraster.subcatchment(), so forget it
    
            rasterFile = f"{srcFile[:-4]}_PixelValueIs{str(val)}.tif"
            tmpVectorFile = f"{srcFile[:-4]}_PixelValueIs{str(val)}.gpkg"
            cmd = 'gdal_polygonize.py {r} -overwrite -b 1 -f "GPKG" {v} OUTPUT {z}'.format(r=rasterFile,v=tmpVectorFile,z=zName)
            os.system(cmd)
            #The above gdal command returns a vector file with multiple entities: one polygon for the envelop, and multiple polygons where region is broken
            #So we remove the envelop (catch_id=0) polygon mnd merge the others 
            tmpVectorLoad = gpd.read_file(tmpVectorFile)
            tmpVectorLoad[f"{zName}"] = tmpVectorLoad[f"{zName}"].astype("string")
            m = tmpVectorLoad[f"{zName}"] == str(val)
            tmpVectorLoadCleared = tmpVectorLoad.loc[m] #returns a gdf without catch_id=0
            union = tmpVectorLoadCleared.unary_union #returns a shapely.geometry
            frames = {'catch_id':[val],
              'geometry' : [union]}
            uniongdf = gpd.GeoDataFrame(frames, crs=f"EPSG:{str(epsgCode)}")
            uniongdf = uniongdf.set_geometry('geometry')
            if os.path.exists(dstFile):
                os.remove(tmpVectorFile)
                uniongdf.to_file(tmpVectorFile)
            else:
                uniongdf.to_file(tmpVectorFile)
            vectorList.append(uniongdf)
            del rasterFile, tmpVectorFile, tmpVectorLoad, tmpVectorLoadCleared, union, uniongdf
    
        #Concatenate each above creater vector layers into a single geodatframe
        df = pd.concat(vectorList)
        gdf = gpd.GeoDataFrame(df, crs=f"EPSG:{str(epsgCode)}")
        gdf = gdf.set_geometry('geometry')
    
        #Write geodataframe to disk
        if os.path.exists(dstFile):
            os.remove(dstFile)
            gdf.to_file(dstFile)
        else:
            gdf.to_file(dstFile)
    
   
    return 



def drop_duplicates_in_gdal_polygonize(inf,ouf):

    """
    function in progress
    Converts raster to polygons with one polygon for a connected region of same pixel value (using gdal_polygonize.py)
    and then clearing duplicates through keeping only duplicated entities that have the greatest value for 'area' field
    """
    
    
    
    #Import libraries
    import geopandas as gpd
    
    inf = f"{tmpDirectory}/StationsMergedResampling_{layer}_{modelRun}_band{str(bandNumber)}_subcatchments.gpkg"
    gdf = gpd.read_file(srcFile)
    gdf.set_crs(f"EPSG:{str(EPSG)}", inplace=True, allow_override=True)
    gdf['area'] = gdf['geometry'].area
    
    list_ids = list(set(gdf['catch_id']))
    
    frames =[]
    for i in list_ids:
        mask = gdf['catch_id'] == int(i)
        gdc = gdf.loc[mask]
        del mask
        max_area = gdc['area'].max()
        mask = gdf['area'] == max_area
        tmp = gdf.loc[mask]
        frames.append(tmp)
        del gdc, tmp
    
    df = pd.concat(frames)
    df.drop(labels='area',axis=1,inplace=True)
    out = gpd.GeoDataFrame(df, crs=f"EPSG:{str(epsgCode)}")
    out.set_geometry('geometry')
    
    ouf = f"{tmpDirectory}/test_subcatchments_noduplicates.gpkg"
    out.to_file(ouf)


    return


def extract_edges(srcFile,*dstFile):

    """
    srcFile: path/to/source/raster/file.tif (can also open .map file or any gdal driver-compatible file format) [string]
    dstFile: [optional] /path/to/destination/file.tif (espg 4326 required)
    
    output : python dictionary with keys 'coordinates', 'values', 'id' i.e. 'coordinates':["xmin ymin xmax ymax", ...], 'value':[...], 'id':[(y,x),...] for each edge cell in srcFile [dict]
    """
    
    #Import libraries
    
    import numpy as np
    import pandas as pd
    from osgeo import gdal
    gdal.UseExceptions()
    gdalDataTypes = {
                  "uint8": 1,
                  "int8": 1,
                  "uint16": 2,
                  "int16": 3,
                  "uint32": 4,
                  "int32": 5,
                  "float32": 6,
                  "float64": 7,
                  "complex64": 10,
                  "complex128": 11
                } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read srcFile and extract info
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    src = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    h,w = src.shape

    #Create empty dictionary to further store results
    
    frames = {'coordinates':[],
              'values':[],
             'id':[]}

    #Create empty array to optionally export further to .tif file
    array = np.empty([h,w])
    array[:] = np.nan

    #Range width-wise (height fix to upper row)

    ymax = uly 
    ymin = uly + yres

    for x in range(w):

        if x == 0:
            xmin = ulx 
            xmax = ulx + xres 
        else:
            xmin = ulx + x * xres 
            xmax = ulx + x * xres + xres 

        frames['coordinates'].append(f"{xmin} {ymin} {xmax} {ymax}")
        frames['values'].append(np.float32(src[0,x])) #-1 since np.array index starts at 0 and default format is float64 which is not compatible with pandas.df float32 format
        frames['id'].append((0,x))

        array[0,x] = np.float32(src[0,x])

    #Range width-wise (height fix to bottom row)

    ymax = uly + h * yres - yres
    ymin = uly + h * yres 

    for x in range(w):

        if x == 0:
            xmin = ulx 
            xmax = ulx + xres
        else:
            xmin = ulx + x * xres 
            xmax = ulx + x * xres + xres

        frames['coordinates'].append(f"{xmin} {ymin} {xmax} {ymax}")
        frames['values'].append(np.float32(src[h-1,x]))
        frames['id'].append((h-1,x))

        array[h-1,x] = np.float32(src[h-1,x])

    #Range height-wise (width fix to left colomn)

    xmin = ulx 
    xmax = ulx + xres

    for y in range(h): 

        if y == 0: 
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres 
            ymin = uly + y * yres + yres 

        frames['coordinates'].append(f"{xmin} {ymin} {xmax} {ymax}")
        frames['values'].append(np.float32(src[y,0]))
        frames['id'].append((y,0))

        array[y,0] = np.float32(src[y,0])

    #Range height-wise (width fix to right colomn)

    xmin = ulx + w * xres - xres
    xmax = ulx + w * xres 

    for y in range(h): 

        if y == 0: 
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres 
            ymin = uly + y * yres + yres 

        frames['coordinates'].append(f"{xmin} {ymin} {xmax} {ymax}")
        frames['values'].append(np.float32(src[y,w-1]))
        frames['id'].append((y,w-1))

        array[y,w-1] = np.float32(src[y,w-1])
    
    #Keep unique values to deal with the 4 redundant cells (one at each corner)
    #out = list(np.unique(tuples))

    #Flush memory 
    r = None

    #Optionnaly export as dstFile raster
    
    if len(dstFile) != 0:
        
        gdalType = gdalDataTypes[array.dtype.name]
        outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, gdalType)
        outBand = outDs.GetRasterBand(1)
        outBand.WriteArray(array)
        outDs.SetGeoTransform(geoT)
        srs = osr.SpatialReference()
        srs.ImportFromEPSG(epsgCode)
        outDs.SetProjection(srs.ExportToWkt())
    
        #Flush memory 
        r = None
        outDs = None
        
    else:
        pass
        
    return frames


def extract_edge_inflows(srcFile,dstFile,epsgCode):

    """
    srcFile: path/to/source/FlowDirection/file.tif [string]

    output : path/to/dst/raster/file.tif where cell=1 if it is an edge inflow and cell=0 else [string]
    """
    
    #Import libraries
    
    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    gdalDataTypes = {
                      "uint8": 1,
                      "int8": 1,
                      "uint16": 2,
                      "int16": 3,
                      "uint32": 4,
                      "int32": 5,
                      "float32": 6,
                      "float64": 7,
                      "complex64": 10,
                      "complex128": 11
                    } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read srcFile and extract info
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    a = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    
    h,w = a.shape

    #Create an empty array populated with NaN values to further store results
    
    array = np.empty([h,w])
    array[:] = np.nan
    
    #Set inflow codes for North, South, East, West streams
    #see D8 algorithm codes: https://pcraster.geo.uu.nl/pcraster/4.4.0/documentation/pcraster_manual/sphinx/secdatbase.html#ldd-data-type
    inflowCodes = {
                    'inflowsN':[np.float32(x) for x in [1,2,3]],
                    'inflowsS':[np.float32(x) for x in [7,8,9]],
                    'inflowsE':[np.float32(x) for x in [7,4,1]],
                    'inflowsW':[np.float32(x) for x in [9,6,3]]
                    }

    #Range width-wise (height fix to upper row)

    ymax = uly 
    ymin = uly + yres

    for x in range(w):

        if x == 0:
            xmin = ulx 
            xmax = ulx + xres 
        else:
            xmin = ulx + x * xres 
            xmax = ulx + x * xres + xres 

        #Find outlets
        flowDirection = np.float32(a[0,x])  
        if flowDirection in inflowCodes['inflowsN']:
            array[0,x] = np.float32(1)
        else:
            array[0,x] = np.float32(np.nan)
        
    #Range width-wise (height fix to bottom row)

    ymax = uly + h * yres - yres
    ymin = uly + h * yres 

    for x in range(w):

        if x == 0:
            xmin = ulx 
            xmax = ulx + xres
        else:
            xmin = ulx + x * xres 
            xmax = ulx + x * xres + xres

        #Find outlets
        flowDirection = np.float32(a[h-1,x])  
        if flowDirection in inflowCodes['inflowsS']:
            array[h-1,x] = np.float32(1)
        else:
            array[h-1,x] = np.float32(np.nan)

    #Range height-wise (width fix to left colomn)

    xmin = ulx 
    xmax = ulx + xres

    for y in range(h): 

        if y == 0: 
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres 
            ymin = uly + y * yres + yres 

        #Find outlets
        flowDirection = np.float32(a[y,0]) 
        if flowDirection in inflowCodes['inflowsW']:
            array[y,0] = np.float32(1)
        else:
            array[y,0] = np.float32(np.nan)

    #Range height-wise (width fix to right colomn)

    xmin = ulx + w * xres - xres
    xmax = ulx + w * xres 

    for y in range(h): 

        if y == 0: 
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres 
            ymin = uly + y * yres + yres 

        #Find outlets
        flowDirection = np.float32(a[y,w-1]) 
        if flowDirection in inflowCodes['inflowsE']:
            array[y,w-1] = np.float32(1)
        else:
            array[y,w-1] = np.float32(np.nan)

    #Export array as dstFile .tif

    gdalType = gdalDataTypes[array.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, gdalType)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(array)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    #Flush memory 
    r = None
    
    return



def extract_edge_outlets(srcFile,dstFile,epsgCode):

    """
    srcFile: path/to/source/FlowDirection/file.tif [string]

    output : path/to/dst/raster/file.map where cell=1 if it is an edge outlet and cell=0 else [string]
    """
    
    #Import libraries
    
    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    gdalDataTypes = {
                      "uint8": 1,
                      "int8": 1,
                      "uint16": 2,
                      "int16": 3,
                      "uint32": 4,
                      "int32": 5,
                      "float32": 6,
                      "float64": 7,
                      "complex64": 10,
                      "complex128": 11
                    } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read srcFile and extract info
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    a = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    
    h,w = a.shape

    #Create an empty array populated with NaN values to further store results
    
    array = np.empty([h,w])
    array[:] = np.nan
    
    #Set outlet codes for North, South, East, West streams
    #see D8 algorithm codes: https://pcraster.geo.uu.nl/pcraster/4.4.0/documentation/pcraster_manual/sphinx/secdatbase.html#ldd-data-type
    outletCodes = {
                    'outletsN':[np.float32(x) for x in [5,7,8,9]],
                    'outletsS':[np.float32(x) for x in [5,1,2,3]],
                    'outletsE':[np.float32(x) for x in [5,9,6,3]],
                    'outletsW':[np.float32(x) for x in [5,7,4,1]]
                    }

    #Range width-wise (height fix to upper row)

    ymax = uly 
    ymin = uly + yres

    for x in range(w):

        if x == 0:
            xmin = ulx 
            xmax = ulx + xres 
        else:
            xmin = ulx + x * xres 
            xmax = ulx + x * xres + xres 

        #Find outlets
        flowDirection = np.float32(a[0,x])  
        if flowDirection in outletCodes['outletsN']:
            array[0,x] = np.float32(1)
        else:
            array[0,x] = np.float32(np.nan)
        
    #Range width-wise (height fix to bottom row)

    ymax = uly + h * yres - yres
    ymin = uly + h * yres 

    for x in range(w):

        if x == 0:
            xmin = ulx 
            xmax = ulx + xres
        else:
            xmin = ulx + x * xres 
            xmax = ulx + x * xres + xres

        #Find outlets
        flowDirection = np.float32(a[h-1,x])  
        if flowDirection in outletCodes['outletsS']:
            array[h-1,x] = np.float32(1)
        else:
            array[h-1,x] = np.float32(np.nan)

    #Range height-wise (width fix to left colomn)

    xmin = ulx 
    xmax = ulx + xres

    for y in range(h): 

        if y == 0: 
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres 
            ymin = uly + y * yres + yres 

        #Find outlets
        flowDirection = np.float32(a[y,0]) 
        if flowDirection in outletCodes['outletsW']:
            array[y,0] = np.float32(1)
        else:
            array[y,0] = np.float32(np.nan)

    #Range height-wise (width fix to right colomn)

    xmin = ulx + w * xres - xres
    xmax = ulx + w * xres 

    for y in range(h): 

        if y == 0: 
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres 
            ymin = uly + y * yres + yres 

        #Find outlets
        flowDirection = np.float32(a[y,w-1]) 
        if flowDirection in outletCodes['outletsE']:
            array[y,w-1] = np.float32(1)
        else:
            array[y,w-1] = np.float32(np.nan)

    #Additional treatment for those outlets that for some reason do not locate on the edges
    #Range again but over all cells and retrieve cells=5

    for y in range(h): 

        if y == 0: #yres is negative and y axis goes x-wise
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
            ymin = uly + y * yres + yres 
        
        for x in range(w):

            if x == 0:
                xmin = ulx 
                xmax = ulx + xres
            else:
                xmin = ulx + x * xres 
                xmax = ulx + x * xres + xres

            flowDirection = np.float32(a[y,x])
            
            if flowDirection == np.float32(5):
                array[y,x] = np.float32(1)
            else:
                array[y,x] = np.float32(np.nan)

    #Export array as dstFile .tif

    gdalType = gdalDataTypes[array.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, gdalType)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(array)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    #Flush memory 
    r = None
    
    return


def significant_outlets(srcFile,maskFile,dstFile,epsgCode,threshold=None):

    """
    Genuinely made to retrieve outlets that have a discharge value at least as big as a filtered accuflux map, the filtering value of which can be increased by the threshold parameter
    
    maskFile: /path/to/src/raster/file/of/outlets.tif #Must be a boolean raster [string]
    srcFile: /path/to/mask/raster/file/of/accuflux/as/threshold.tif #Must be a scalar raster, have same dimension and resolution as maskFile & NoDataValue must be set to 'none' [string]
    threshold: [optionnal] scalar value that can be set to increase the significance level of outlets retrieved higher than the proper threshold of acculfux [scalar]

    dstFile: /path/to/output/raster/of/significant/outlets.tif [string]
    """

    #Import libraries
    
    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    gdalDataTypes = {
                      "uint8": 1,
                      "int8": 1,
                      "uint16": 2,
                      "int16": 3,
                      "uint32": 4,
                      "int32": 5,
                      "float32": 6,
                      "float64": 7,
                      "complex64": 10,
                      "complex128": 11
                    } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read srcFile and extract info
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    src = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    
    h,w = src.shape

    r = None

    #Read maskFile and extract info
    
    r = gdal.Open(maskFile)
    band = r.GetRasterBand(1)
    msk = band.ReadAsArray().astype(float)   

    r = None

    #Create an empty array populated with NaN values to further store results
    
    array = np.empty([h,w])
    array[:] = np.float32(np.nan)

    #Range over cells and retrieve cells on condition

    for y in range(h): 

        if y == 0: #yres is negative and y axis goes x-wise
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
            ymin = uly + y * yres + yres 
        
        for x in range(w):

            if x == 0:
                xmin = ulx 
                xmax = ulx + xres
            else:
                xmin = ulx + x * xres 
                xmax = ulx + x * xres + xres

            if (xmin >= xmax) | (ymin >= ymax):
                print("Error (x,y)",(x,y))
            else:
                pass

            #Set float values
            accuflux = np.float32(src[y,x])
            outlets = np.float32(msk[y,x])
        
            if threshold is None:
                scalar = np.float32(1)
            else:
                scalar = np.float32(threshold)

            #Retieve outlets on condition

            if accuflux*outlets == np.float32(np.nan):
                array[y,x] = np.float32(np.nan) 
            elif accuflux*outlets < scalar:
                array[y,x] = np.float32(np.nan) 
            else:
                array[y,x] = accuflux*outlets
            
    #Export array as dstFile .tif

    gdalType = gdalDataTypes[array.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 1, gdalType)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(array)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    return

def compute_estress(pristRaster,distRaster,estressRaster,epsgCode,stressLevel,useStressLevel):

    """
    pristRaster: /path/to/pristine/flow/raster/file.tif #Resampled pristine flow raster [string]
    distRaster: /path/to/distrubed/flow/raster/file.tif #Resampled disturbed flow raster [string]
    #prist and dist rasters must be same dimension and projection
    stressLevel: allowed flow distrubance value as a fraction of pristValue # 0 < stress < 1 [float] 
    useStressLevel: specify wheher to use the stressLevel and compute an exceedance raster of 0s and 1s (True), or not to use it and compute a raw flow variation raster (False) 
    
    estressRaster: /path/to/output/estress/raster/file.tif [string]
    #where cell=(distRaster-pristRaster)/pristRaster if stress parameter is left unset 
    #where cell=0 if pristRaster*(1-stress) <= (distRaster-pristRaster)/pristRaster <= pristRaster*(1+stress), cell=1 else 
    """

    #Import libraries
    
    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()

    gdalDataTypes = {
                      "uint8": 1,
                      "int8": 1,
                      "uint16": 2,
                      "int16": 3,
                      "uint32": 4,
                      "int32": 5,
                      "float32": 6,
                      "float64": 7,
                      "complex64": 10,
                      "complex128": 11
                    } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Read pristRaster and extract geoT info
    
    r = gdal.Open(pristRaster)
    band = r.GetRasterBand(1)
    prist = band.ReadAsArray().astype(float)
    
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    
    h,w = prist.shape

    r = None

    #Read distRaster and extract info
    
    r = gdal.Open(distRaster)
    band = r.GetRasterBand(1)
    dist = band.ReadAsArray().astype(float)   

    r = None

    #Create an empty array populated with NaN values to further store results
    
    array = np.empty([h,w])
    array[:] = np.float32(np.nan)

    #Range over cells and retrieve cells on condition

    for y in range(h): 

        if y == 0: #yres is negative and y axis goes x-wise
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
            ymin = uly + y * yres + yres 
        
        for x in range(w):

            if x == 0:
                xmin = ulx 
                xmax = ulx + xres
            else:
                xmin = ulx + x * xres 
                xmax = ulx + x * xres + xres

            if (xmin >= xmax) | (ymin >= ymax):
                print("Error (x,y)",(x,y))
            else:
                pass

            #Set float values
            pristValue = np.float32(prist[y,x])
            distValue = np.float32(dist[y,x])
            estress = np.float32((distValue - pristValue)/pristValue)
            
            #Compute ESTRESS indicator
            
            if useStressLevel is False:
                array[y,x] = estress
            else: 
                if np.isnan(estress):
                    array[y,x] = np.float32(np.nan)
                elif (estress <= np.float32(1)+np.float32(stressLevel)) | (estress >= np.float32(1)-np.float32(stressLevel)):
                    array[y,x] = np.float32(1)
                else:
                    array[y,x] = np.float32(0)

            
    #Export array as estressRaster .tif

    gdalType = gdalDataTypes[array.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').Create(estressRaster, w, h, 1, gdalType)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(array)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    return

def cells_to_points(dictCells,epsgCode,*dstFile):

    """
    dictCells: python dictionary containing keys 'coordinates' and 'values' i.e. 'coordinates':["xmin ymin xmax ymax", ...], 'value':[...] [dict]
    dstFile [optional]: /path/to/vector/layer.gpkg [string]

    output: geodataframe with cells turned to points through centroid [geopandas.DataFrame]
    """

    #Import libraries
    
    import pandas as pd
    from shapely.geometry import Point
    import geopandas as gpd

    #Convert dictCells to dataframe

    df = pd.DataFrame(dictCells)
    df['coordinates'] = df['coordinates'].astype('string')
    listCoordinates = list(df['coordinates'])
    listxmin = []
    listxmax = []
    listymin = []
    listymax = []
    for elem in listCoordinates:
        tmp = elem.split(" ")
        listxmin.append(tmp[0])
        listymin.append(tmp[1])
        listxmax.append(tmp[2])
        listymax.append(tmp[3])
    df['xmin'] = listxmin
    df['ymin'] = listymin
    df['xmax'] = listxmax
    df['ymax'] = listymax
    
    #Create columns for center x coord and center y coord"
    
    df['xmin'] = df['xmin'].astype('float')
    df['xmax'] = df['xmax'].astype('float')
    df['ymin'] = df['ymin'].astype('float')
    df['ymax'] = df['ymax'].astype('float')
    df['xcenter'] = (df['xmin'] + df['xmax']) / 2
    df['ycenter'] = (df['ymin'] + df['ymax']) / 2
    
    #Drop useless colmumns
    
    fields = ['values','xcenter','ycenter']
    dfc = df[fields]

    df = None

    #Reset index
    
    dfc = dfc.reset_index()

    #Convert dataframe to geodataframe
    
    geometry = [Point(xy) for xy in zip(dfc['xcenter'], dfc['ycenter'])]
    tmp = dfc.drop(['xcenter', 'ycenter','index'], axis=1)
    gdf = gpd.GeoDataFrame(tmp, crs=f"EPSG:{str(epsgCode)}", geometry=geometry)

    tmp = None
    dfc = None
    
    #Optionnaly export to vector layer (format GPKG)

    if len(dstFile) != 0:
        gdf.to_file(dstFile)
    else:
        pass

    
    return gdf 


def cells_to_polygons(dictCells,epsgCode,*dstFile):

    """
    dictCells: python dictionary containing keys 'coordinates' and 'values' i.e. 'coordinates':["xmin ymin xmax ymax", ...], 'value':[...] [dict]
    dstFile [optional]: /path/to/vector/layer.gpkg [string]

    output: geodataframe with cells turned to polygons [geopandas.DataFrame]
    """

    #Import libraries
    
    import pandas as pd
    from shapely import wkt
    import geopandas as gpd

    #Add a WKT key to dictionnary
    
    tmp = {'wkt':[]}
    dictCells.update(tmp)

    for elem in dictCells['coordinates']: #xmin ymin xmax ymax
        temp = elem.split()
        dictCells['wkt'].append(f"POLYGON (({temp[0]} {temp[1]},{temp[2]} {temp[1]},{temp[2]} {temp[3]},{temp[0]} {temp[3]},{temp[0]} {temp[1]}))")
    
    #Note on WKT syntax: the first coordinate needs to be repeated at the end of the sequence to close the ring + sequence to call coordinates is [0,1] [2,1] [2,3] {0,3] [0,1]

    #Convert to pandas dataframe
    
    df = pd.DataFrame(dictCells)

    #Convert to geopandas geodataframe

    df['geometry'] = df['wkt'].apply(wkt.loads)
    df.drop('wkt', axis=1, inplace=True)
    df.drop('coordinates', axis=1, inplace=True)
    gdf = gpd.GeoDataFrame(df, crs=f"EPSG:{str(epsgCode)}", geometry='geometry')

    #Optionnaly export to vector layer (format GPKG)

    if len(dstFile) != 0:
        gdf.to_file(dstFile)
    else:
        pass
        

    return gdf
    

def join_points_to_pixels(pointsFile,pointsLayer,rasterFile,epsgCode,dstFile):
    
    """
    Originally made to extract resampled discharge values based on their nearest position to gauge stations.
    Returns a geodataframe where geometry is the one of the closest-to-stations pixels and attributes are from both source files
    
    pointsFile: /path/to/vector/points/file.gpkg [string]
    pointsLayer: name of the column in pointsFile that will be retrieved in output [string]
    rasterFile: /path/to/raster/file.tif [string]
    epsgCode: EPSG Code of both pointsFile and rasterFile [int]
    
    dstFile: /path/to/destination/file.gpkg that is the point geometry geodataframe with merged attributes and coordinates taken from rasterFile's pixels [string]       
    """
    
    import geopandas as gpd
    from shapely import wkt
    
    #Convert rasterFile to geoDataFrame and create a 'uid' column
    raster = extract_cellsValues(rasterFile)
    gdf_raster = cells_to_points(raster,epsgCode)
    uids  = [i for i in range(len(gdf_raster))]
    gdf_raster['uid'] = uids
    
    #Exclude NaN values from gdf_raster
    gdf_raster_noNaN = gdf_raster.dropna(axis=0,subset='values')
    
    #Open pointsFile and cut dataframe to only the following columns ['pointsLayer','geometry','code_station'] 
    points = gpd.read_file(pointsFile)
    gdf_points = points[[f"{str(pointsLayer)}",'geometry','code_station']]
    
    #Spatial joining and merging
    join = gpd.sjoin_nearest(gdf_points,gdf_raster_noNaN,how='left',lsuffix='points', rsuffix='raster')
    
    #Retrieve the coordinates of cooresponding pixels in rasterFile
    gdf_raster_geom = gdf_raster[['geometry','uid']]
    merge = join.merge(gdf_raster_geom,left_on='uid',right_on='uid',how='left',suffixes=('_points', '_raster'))
    
    #Convert above dataframe to geodataframe based on raster geometry
    merge['station_coordinates'] = merge['geometry_points'] 
    merge['station_coordinates'] = merge['station_coordinates'].astype("string")
    merge.drop(['geometry_points'], axis=1, inplace=True)
    merge['geometry'] = merge['geometry_raster']
    merge.drop('geometry_raster', axis=1, inplace=True)
    gdf = gpd.GeoDataFrame(merge, crs=f"EPSG:{str(epsgCode)}", geometry='geometry')
    
    #Write gdf to disk
    gdf.to_file(dstFile)  
    
    return gdf


def add_pixelIndexing(srcFile,dstFile,epsgCode):

    """
    
    srcFile: /path/to/source/raster.tif must be a single band raster [string]
    dstFile: /path/to/destination/raster.tif [string]
    
    output: raster srcFile is added a second band with pixel value = pixel index of the overlying numpy.array associated with band 1
    Note that coordinates (x,y) cannot be written as a compatible gdalDataType, so a each 2-dimension coordinate is converted to a 1-dimension index
    The algorithm reads the raster array from upper row, then column-wise from left column. So the pattern for 1-dimension indexing is, for e.g. an array.shape = 4,4:
    0  1  2  3
    4  5  6  7
    8  9  10 11
    12 13 14 15
    
    """

    import numpy as np
    from osgeo import gdal, osr
    gdal.UseExceptions()
    gdalDataTypes = {
                  "uint8": 1,
                  "int8": 1,
                  "uint16": 2,
                  "int16": 3,
                  "uint32": 4,
                  "int32": 5,
                  "float32": 6,
                  "float64": 7,
                  "complex64": 10,
                  "complex128": 11
                } #see: https://borealperspectives.org/2014/01/16/data-type-mapping-when-using-pythongdal-to-write-numpy-arrays-to-geotiff/

    #Open rasters
    
    r = gdal.Open(srcFile)
    band = r.GetRasterBand(1)
    band1 = band.ReadAsArray().astype(float)
    ulx, xres, xskew, uly, yskew, yres = r.GetGeoTransform()
    geoT = r.GetGeoTransform()
    h,w = band1.shape

    #Create an empty array populated with NaN values to further store results

    band2 = np.empty([h,w])
    band2[:] = np.float32(np.nan)
    index = 0
    
    #Range over cells and retrieve cells on condition

    for y in range(h): 

        if y == 0: #yres is negative and y axis goes x-wise
            ymax = uly 
            ymin = uly + yres
        else:
            ymax = uly + y * yres #for last iteration, h = h-1 because of range() behavior
            ymin = uly + y * yres + yres 
        
        for x in range(w):

            if x == 0:
                xmin = ulx 
                xmax = ulx + xres
            else:
                xmin = ulx + x * xres 
                xmax = ulx + x * xres + xres

            if (xmin >= xmax) | (ymin >= ymax):
                print("Error (x,y)",(x,y))
            else:
                pass
            
            band2[y,x] = np.int32(index)
            index += 1
    
    
    #Export array as dstFile .tif

    #Write first band to disk
    gdalType = gdalDataTypes[band1.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').Create(dstFile, w, h, 2, gdalType)
    outBand = outDs.GetRasterBand(1)
    outBand.WriteArray(band1)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    #Write second band to disk
    sourceDS = gdal.Open(dstFile)
    gdalType = gdalDataTypes[band2.dtype.name]
    outDs = gdal.GetDriverByName('GTiff').CreateCopy(dstFile, sourceDS, strict=0)
    outBand = outDs.GetRasterBand(2)
    outBand.WriteArray(band2)
    outDs.SetGeoTransform(geoT)
    srs = osr.SpatialReference()
    srs.ImportFromEPSG(epsgCode)
    outDs.SetProjection(srs.ExportToWkt())
    outDs = None

    #Flush
    r = None

    return 



    
























    






